{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertForMaskedLM, BertTokenizer, BertModel\n",
    "\n",
    "import csv\n",
    "import logging\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import preprocessing\n",
    "import gzip\n",
    "import sys\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from scipy.spatial import distance\n",
    "import random\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from sklearn.mixture import GaussianMixture as GMM\n",
    "from sklearn.mixture import BayesianGaussianMixture\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "import os\n",
    "import random\n",
    "from scipy.cluster import hierarchy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tabulate import tabulate\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.spatial import distance_matrix\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from adjustText import adjust_text\n",
    "\n",
    "\n",
    "from modules.modules_ import *\n",
    "from clustering.gmm import *\n",
    "from clustering.kmeans import *\n",
    "from embeddings.extract_bert import *\n",
    "from embeddings.extract_count import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('analysis_count_1.pickle', 'rb') as handle:\n",
    "    bf_count = pickle.load(handle)\n",
    "\n",
    "with open('analysis_gmm_20.pickle', 'rb') as handle:\n",
    "    bf_bert = pickle.load(handle)"
   ]
  },
  {
   "source": [
    "# Onomacluster"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relations = {\n",
    "    \"chamaco\": [\"chico\", \"chamaco\", \"pibe\"], \n",
    "    \"vidriera\": [\"vidriera\", \"escaparate\"],\n",
    "    \"volante\" : [\"volante\", \"tim√≥n\"],\n",
    "    \"sindicar\" : [\"sindicar\", \"acusar\"],\n",
    "    \"guagua\" : [\"guagua\", \"colectivo\"],\n",
    "    \"amarrar\" : [\"amarrar\", \"atar\"],\n",
    "    \"cartera\" : [\"cartera\", \"bolso\"],\n",
    "    \"coche\" : [\"coche\", \"carro\"],\n",
    "    \"pollera\" : [\"pollera\", \"falda\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onomafit = {}\n",
    "\n",
    "for target_word in relations:\n",
    "    print(target_word)\n",
    "    pathTestSentences = \"context_tables_final/context_table_\" + target_word + \".csv\"\n",
    "    sentences = read_sentences(pathTestSentences, variety=None, exact_word=None)\n",
    "    vectors = list(embeddings[target_word])\n",
    "    \n",
    "    target_vectors_count = len(vectors)\n",
    "    \n",
    "    n_clusters =bf_bert[target_word][\"n_clusters_silhouette\"]\n",
    "\n",
    "\n",
    "    kmeans = KMeans(n_clusters=n_clusters).fit(np.array(vectors))\n",
    "    orig_labels = kmeans.labels_    \n",
    "\n",
    "    if n_clusters < 3:\n",
    "        n_clusters *= 2\n",
    "\n",
    "    kmeans = KMeans(n_clusters=n_clusters).fit(np.array(vectors))\n",
    "    y = kmeans.labels_\n",
    "\n",
    "\n",
    "    centroids = kmeans.cluster_centers_\n",
    "\n",
    "    dist = pd.DataFrame(cdist(centroids, centroids, metric=\"cosine\"))\n",
    "    print(dist)\n",
    "\n",
    "    indices = {i:i for i in range(len(dist))}\n",
    "\n",
    "    for i, row in dist.iterrows():\n",
    "        for j in range(i + 1, len(row)):\n",
    "            if row[j] < 0.10:\n",
    "                indices[j] = i\n",
    "\n",
    "    for k,v in indices.items():\n",
    "        indices[k] = indices[v]\n",
    "\n",
    "    graph = nx.read_gpickle('../graphs/' + target_word)\n",
    "    wug_clusters = get_clusters(graph)\n",
    "\n",
    "    gold_labels = [get_cluster(wug_clusters, sentence[\"node\"]) for sentence in sentences]\n",
    "    labels = [indices[i] for i in y]\n",
    "\n",
    "    orig_sil = 0\n",
    "    if len(set(y)) > 1:\n",
    "        orig_sil = silhouette_score(vectors, orig_labels)\n",
    "\n",
    "    sil = 0\n",
    "    if len(set(labels)) > 1:\n",
    "        sil = silhouette_score(vectors, labels)\n",
    "\n",
    "    labels = labels[0:target_vectors_count]\n",
    "    score = adjusted_rand_score(sorted(gold_labels), sorted(labels))\n",
    "    orig_score = adjusted_rand_score(sorted(gold_labels), sorted(orig_labels[0:target_vectors_count]))\n",
    "    purity = purity_score(gold_labels, labels)\n",
    "    orig_purity = purity_score(gold_labels, orig_labels[0:target_vectors_count])\n",
    "    \n",
    "    onomafit[target_word] = {\n",
    "        \"n_clusters_onoma\":len(set(indices.values())), \n",
    "        \"score\":score, \n",
    "        \"purity\": purity, \n",
    "        \"silhouette\" : sil, \n",
    "        \"orig_sil\" : orig_sil, \n",
    "        \"orig_score\": orig_score, \n",
    "        \"orig_pur\" : orig_purity\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "scores = []\n",
    "purities = []\n",
    "for k in onoma_bert.keys():\n",
    "    if round(onomafit[k][\"silhouette\"] - onomafit[k][\"orig_sil\"],2) > 0.20 or onomafit[k][\"n_clusters_onoma\"] == 1:\n",
    "        l.append([k, onomafit[k][\"n_clusters_onoma\"], onomafit[k][\"score\"], onomafit[k][\"purity\"], round(onomafit[k][\"silhouette\"],2)])\n",
    "        scores.append(onomafit[k][\"score\"])\n",
    "        purities.append(onomafit[k][\"purity\"])\n",
    "    else:\n",
    "        l.append([k, bf_bert[k][\"n_clusters_silhouette\"], onomafit[k][\"orig_score\"], onomafit[k][\"orig_pur\"], round(onomafit[k][\"orig_sil\"],2)])\n",
    "        scores.append(onomafit[k][\"orig_score\"])\n",
    "        purities.append(onomafit[k][\"orig_pur\"])\n",
    "\n",
    "l.append([\"MEAN\", \"-\", sum(scores)/len(scores), sum(purities)/len(purities),\"-\"])\n",
    "\n",
    "print(tabulate(l, headers=[\"Lemma\", \"BEST N Cluster\", \"BEST ARI\", \"BEST Purity\", \"BEST Silhouette\"]))"
   ]
  },
  {
   "source": [
    "# Visualize Bert Embeddings"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_word = \"volante\"\n",
    "pathTestSentences = \"context_tables_final/context_table_\"+target_word+\".csv\"\n",
    "sentences = read_sentences(pathTestSentences)\n",
    "vectors = extract_embeddings(sentences)\n",
    "\n",
    "kmeans = KMeans(n_clusters=onomafit[target_word][\"n_clusters_onoma\"]).fit(vectors)\n",
    "labels = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_model = TSNE(n_components=2,early_exaggeration=12,verbose=0,metric='cosine',init='pca',n_iter=2500)\n",
    "vectors_2d = tsne_model.fit_transform(vectors)\n",
    "\n",
    "fig = plt.figure(figsize=(14,7))\n",
    "alltexts = list()\n",
    "labeled_points = []\n",
    "\n",
    "\n",
    "tws = [sentence[\"target_word\"] for sentence in sentences]\n",
    "for i, txt in enumerate(tws):\n",
    "    plt.scatter(vectors_2d[i,0], vectors_2d[i,1],s=0)\n",
    "    currtext = plt.text(vectors_2d[i,0], vectors_2d[i,1], txt, family='sans-serif')\n",
    "    alltexts.append(currtext)\n",
    "    \n",
    "numiters = adjust_text(alltexts, autoalign=False, lim=50)\n",
    "plt.show()\n",
    "\n",
    "palette = sns.color_palette(\"bright\", len(set(labels)))\n",
    "fig = plt.figure(figsize=(14,7))\n",
    "sns.scatterplot(vectors_2d[:,0], vectors_2d[:,1], hue=labels, legend='full', palette=palette)\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "# Compare Best Models (BERT and Count)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_num_clusters = 1\n",
    "max_num_clusters = 8\n",
    "\n",
    "best_fit = {}\n",
    "\n",
    "vectors_path = \"model_serialization/vectors/\"\n",
    "\n",
    "with open(vectors_path + 'all_w2i.pickle', 'rb') as handle:\n",
    "    w2i = pickle.load(handle)\n",
    "\n",
    "with open(vectors_path + 'all_freq_list.pickle', 'rb') as handle:\n",
    "    freq_list = pickle.load(handle)\n",
    "\n",
    "\n",
    "for target_word, variety in varieties.items():\n",
    "    try:\n",
    "        graph = nx.read_gpickle('../graphs/' + target_word)\n",
    "        wug_clusters = get_clusters(graph)\n",
    "    except:\n",
    "        continue  \n",
    "    \n",
    "    pathTestSentences = \"context_tables_final/context_table_\" + target_word + \".csv\"\n",
    "\n",
    "    vectors, sentences = compute_vectors(target_word, pathTestSentences, variety, vectors_path, freq_list, w2i, joint=True, exact_target_word=False)\n",
    "    \n",
    "    vectors = normalize(vectors, norm='l2', axis=1)\n",
    "\n",
    "\n",
    "    nodes = pd.read_csv(pathTestSentences, sep=\"\\t\")[\"identifiers\"].tolist()\n",
    "    gold_labels = [get_cluster(wug_clusters, node) for node in nodes]\n",
    "\n",
    "    n_clusters = min_num_clusters\n",
    "    \n",
    "    score = -1.0\n",
    "    silhouette = -1.0\n",
    "    best_silhouette = -1.0\n",
    "    best_cluster_acc = -1.0\n",
    "\n",
    "    best_n = 1\n",
    "    best_n_sil = 1\n",
    "    best_n_cluster_acc = 1\n",
    "\n",
    "    while n_clusters < max_num_clusters:\n",
    "        try:\n",
    "            km = KMeans(n_clusters=n_clusters)\n",
    "            km.fit(vectors)\n",
    "            labels = km.labels_\n",
    "            \n",
    "            current_score = adjusted_rand_score(sorted(gold_labels), sorted(labels))\n",
    "            if n_clusters > 1:\n",
    "                silhouette = silhouette_score(vectors, labels)\n",
    "            cluster_acc = cluster_accuracy(gold_labels, labels)\n",
    "\n",
    "            if current_score > score:\n",
    "                score = current_score\n",
    "                best_n = n_clusters\n",
    "\n",
    "            if silhouette > best_silhouette:\n",
    "                best_silhouette = silhouette\n",
    "                best_n_sil = n_clusters\n",
    "            \n",
    "            if cluster_acc > best_cluster_acc:\n",
    "                best_cluster_acc = cluster_acc\n",
    "                best_n_cluster_acc = n_clusters\n",
    "\n",
    "            n_clusters += 1\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "\n",
    "        best_fit[target_word] = {\"n_clusters\" : best_n, \"score\" : score, \"n_clusters_silhouette\" : best_n_sil, \"silhouette_score\" : best_silhouette, \"cluster_accuracy\": best_cluster_acc, \"n_cluster_acc\": best_n_cluster_acc}\n",
    "\n",
    "with open('analysis_count_1.pickle', 'wb') as handle:\n",
    "    pickle.dump(best_fit, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_num_clusters = 1\n",
    "max_num_clusters = 8\n",
    "\n",
    "best_fit = {}\n",
    "\n",
    "for target_word, variety in varieties.items():\n",
    "    try:\n",
    "        graph = nx.read_gpickle('../graphs/' + target_word)\n",
    "        wug_clusters = get_clusters(graph)\n",
    "    except:\n",
    "        continue  \n",
    "    \n",
    "    pathTestSentences = \"context_tables_final/context_table_\" + target_word + \".csv\"\n",
    "\n",
    "    sentences = read_sentences(pathTestSentences)\n",
    "\n",
    "    vectors = extract_embeddings(sentences)\n",
    "\n",
    "    nodes = pd.read_csv(pathTestSentences, sep=\"\\t\")[\"identifiers\"].tolist()\n",
    "    gold_labels = [get_cluster(wug_clusters, node) for node in nodes]\n",
    "\n",
    "    n_clusters = min_num_clusters\n",
    "    \n",
    "    score = -1.0\n",
    "    silhouette = -1.0\n",
    "    best_silhouette = -1.0\n",
    "    best_cluster_acc = -1.0\n",
    "\n",
    "    best_n = 1\n",
    "    best_n_sil = 1\n",
    "    best_n_cluster_acc = 1\n",
    "\n",
    "    while n_clusters < max_num_clusters:\n",
    "        try:\n",
    "            model = get_best_gmm(vectors)\n",
    "            \n",
    "            current_score = adjusted_rand_score(sorted(gold_labels), sorted(labels))\n",
    "            if n_clusters > 1:\n",
    "                silhouette = silhouette_score(vectors, labels)\n",
    "                cluster_acc = purity_score(gold_labels, labels)\n",
    "\n",
    "            if current_score > score:\n",
    "                score = current_score\n",
    "                best_n = n_clusters\n",
    "\n",
    "            if silhouette > best_silhouette:\n",
    "                best_silhouette = silhouette\n",
    "                best_n_sil = n_clusters\n",
    "            \n",
    "            if cluster_acc > best_cluster_acc:\n",
    "                best_cluster_acc = cluster_acc\n",
    "                best_n_cluster_acc = n_clusters\n",
    "\n",
    "            n_clusters += 1\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "        \n",
    "        best_fit[target_word] = {\"n_clusters\" : best_n, \"score\" : score, \"n_clusters_silhouette\" : best_n_sil, \"silhouette_score\" : best_silhouette, \"cluster_accuracy\": best_cluster_acc, \"n_cluster_acc\": best_n_cluster_acc}\n",
    "\n",
    "with open('analysis_gmm_20.pickle', 'wb') as handle:\n",
    "    pickle.dump(best_fit, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "source": [
    "# Visualize Statistics"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_statistics(best_fit):\n",
    "    fig, ax = plt.subplots(figsize=(14,7), squeeze=True)\n",
    "    words = [k for k in best_fit.keys()]\n",
    "\n",
    "    aris = [best_fit[w][\"score\"] for w in words]\n",
    "    silhouettes = [best_fit[w][\"silhouette_score\"] for w in words]\n",
    "    accuracies = [best_fit[w][\"cluster_accuracy\"] for w in words]\n",
    "\n",
    "    plt.plot(words, aris, label=\"ARI\")\n",
    "    plt.plot(words, silhouettes, label=\"Silhouette\")\n",
    "    plt.plot(words, accuracies, label=\"Purity\")\n",
    "    plt.xlabel('Target Words')\n",
    "    plt.legend()\n",
    "    plt.xticks(rotation=65)\n",
    "    plt.ylabel('Statistics')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "def compare_stats(best_fit, best_fit_other, field, desc1, desc2, field_desc, path=None):\n",
    "    fig, ax = plt.subplots(figsize=(14,7), squeeze=True)\n",
    "    words = [k for k in best_fit.keys()]\n",
    "    words = words[0:-1]\n",
    "\n",
    "    y1 = [best_fit[w][field] for w in words]\n",
    "    y2 = [best_fit_other[w][field] for w in words]\n",
    "\n",
    "    compare_lists(y1, y2, words, desc1, desc2,field_desc, path)\n",
    "\n",
    "def compare_lists(y1, y2, words, desc1, desc2, field_desc, path=None):\n",
    "    fig, ax = plt.subplots(figsize=(14,7), squeeze=True)\n",
    "    \n",
    "    plt.plot(words, y1, label=desc1)\n",
    "    plt.plot(words, y2, label=desc2)\n",
    "\n",
    "    plt.xlabel('Target Words')\n",
    "    plt.legend()\n",
    "    plt.xticks(rotation=65)\n",
    "    plt.ylabel(field_desc)\n",
    "    plt.grid()\n",
    "    if path == None:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.savefig(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_statistics(bf_count)\n",
    "compare_stats(bf_bert, bf_count, \"silhouette_score\", \"BERT Embeddings\", \"Count Vectors\", \"Silhouette Score\", \"model_vis/plots/sil.png\")\n"
   ]
  }
 ]
}